{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11223852,"sourceType":"datasetVersion","datasetId":7009800}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##  **T5 from Scratch**","metadata":{}},{"cell_type":"code","source":"import torch \nif torch.cuda.is_available():\n    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n    print(\"Memory Allocated:\", round(torch.cuda.memory_allocated(0) / 1e9, 2), \"GB\")\n    print(\"Memory Cached:\", round(torch.cuda.memory_reserved(0) / 1e9, 2), \"GB\")\n    print(\"CUDA Version:\", torch.version.cuda)\n    print(\"GPU Count:\", torch.cuda.device_count())\nelse:\n    print(\"No GPU detected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:32.711331Z","iopub.execute_input":"2025-03-31T10:53:32.711659Z","iopub.status.idle":"2025-03-31T10:53:32.719650Z","shell.execute_reply.started":"2025-03-31T10:53:32.711628Z","shell.execute_reply":"2025-03-31T10:53:32.719018Z"}},"outputs":[{"name":"stdout","text":"GPU Name: Tesla P100-PCIE-16GB\nMemory Allocated: 1.2 GB\nMemory Cached: 4.39 GB\nCUDA Version: 12.1\nGPU Count: 1\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:33.791264Z","iopub.execute_input":"2025-03-31T10:53:33.791593Z","iopub.status.idle":"2025-03-31T10:53:33.795333Z","shell.execute_reply.started":"2025-03-31T10:53:33.791565Z","shell.execute_reply":"2025-03-31T10:53:33.794549Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"import math\nimport time\nimport os\nimport random\nimport numpy as np\nfrom tqdm.notebook import tqdm \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:36.050815Z","iopub.execute_input":"2025-03-31T10:53:36.051106Z","iopub.status.idle":"2025-03-31T10:53:36.054620Z","shell.execute_reply.started":"2025-03-31T10:53:36.051082Z","shell.execute_reply":"2025-03-31T10:53:36.053777Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"\nclass Config:\n    # Data\n    data_file = \"/kaggle/input/taylorswift/taylorswift.txt\"     # Input text file path (MUST EXIST)\n\n    # --- SentencePiece Tokenizer Settings ---\n    sp_model_prefix = 'ts_spm_bpe' # Prefix for saving the trained SentencePiece model files (.model, .vocab)\n    sp_vocab_size = 4000          # Desired vocabulary size (tune this)\n    sp_model_type = 'bpe'         # Model type: 'bpe', 'unigram', 'char', or 'word'\n    sp_pad_id = 3                 # Ensure this matches the ID used in SP training\n    sp_pad_piece = \"<pad>\"        # Representation of the padding token\n\n    vocab_size = None \n    d_model = 512\n    nhead = 8\n    num_encoder_layers = 6\n    num_decoder_layers = 6\n    dim_feedforward = 2048\n    dropout = 0.1\n    max_seq_len = 128\n    relative_attention_num_buckets = 32\n\n    batch_size = 32\n    learning_rate = 1e-4\n    epochs = 30        # << INCREASED EPOCHS\n    clip_grad_norm = 1.0\n\n    seed = 42\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:38.971316Z","iopub.execute_input":"2025-03-31T10:53:38.971598Z","iopub.status.idle":"2025-03-31T10:53:38.975985Z","shell.execute_reply.started":"2025-03-31T10:53:38.971577Z","shell.execute_reply":"2025-03-31T10:53:38.975113Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"import sentencepiece as spm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:41.850974Z","iopub.execute_input":"2025-03-31T10:53:41.851259Z","iopub.status.idle":"2025-03-31T10:53:41.854509Z","shell.execute_reply.started":"2025-03-31T10:53:41.851238Z","shell.execute_reply":"2025-03-31T10:53:41.853612Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"config = Config()\n\nrandom.seed(config.seed)\nnp.random.seed(config.seed)\ntorch.manual_seed(config.seed)\nif config.device.type == 'cuda':\n    torch.cuda.manual_seed_all(config.seed) # Seed all GPUs\n    # Ensure deterministic algorithms for reproducibility (can impact performance slightly)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nprint(f\"Using device: {config.device}\")\nprint(f\"Using data file: {config.data_file}\")\nprint(f\"Tokenizer: SentencePiece ({config.sp_model_type.upper()}), Target Vocab Size: {config.sp_vocab_size}\")\nprint(f\"Current Config:\")\nprint(f\"  d_model       = {config.d_model}\")\nprint(f\"  nhead         = {config.nhead}\")\nprint(f\"  num_layers    = {config.num_encoder_layers} (enc/dec)\")\nprint(f\"  batch_size    = {config.batch_size}\")\nprint(f\"  max_seq_len   = {config.max_seq_len} (tokens)\")\nprint(f\"  learning_rate = {config.learning_rate}\")\nprint(f\"  epochs        = {config.epochs}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:42.211108Z","iopub.execute_input":"2025-03-31T10:53:42.211337Z","iopub.status.idle":"2025-03-31T10:53:42.220059Z","shell.execute_reply.started":"2025-03-31T10:53:42.211317Z","shell.execute_reply":"2025-03-31T10:53:42.219437Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nUsing data file: /kaggle/input/taylorswift/taylorswift.txt\nTokenizer: SentencePiece (BPE), Target Vocab Size: 4000\nCurrent Config:\n  d_model       = 512\n  nhead         = 8\n  num_layers    = 6 (enc/dec)\n  batch_size    = 32\n  max_seq_len   = 128 (tokens)\n  learning_rate = 0.0001\n  epochs        = 30\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"### 4.0 Train SentencePiece Tokenizer (if model doesn't exist)","metadata":{}},{"cell_type":"code","source":"data_filename = config.data_file\nif not os.path.exists(data_filename):\n    print(f\"ERROR: Data file '{data_filename}' not found!\")\n    print(\"Please ensure the file exists in the same directory as this notebook.\")\n    # Stop execution if file is missing\n    raise FileNotFoundError(f\"Required data file not found: {data_filename}\")\nelse:\n    print(f\"Data file '{data_filename}' found. Size: {os.path.getsize(data_filename):,} bytes.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:53:49.730960Z","iopub.execute_input":"2025-03-31T10:53:49.731247Z","iopub.status.idle":"2025-03-31T10:53:49.745629Z","shell.execute_reply.started":"2025-03-31T10:53:49.731224Z","shell.execute_reply":"2025-03-31T10:53:49.744953Z"}},"outputs":[{"name":"stdout","text":"Data file '/kaggle/input/taylorswift/taylorswift.txt' found. Size: 186,754 bytes.\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"\nsp_model_file = f\"{config.sp_model_prefix}.model\"\n\nif not os.path.exists(sp_model_file):\n    print(f\"SentencePiece model not found at '{sp_model_file}'. Training...\")\n    if not os.path.exists(config.data_file):\n         raise FileNotFoundError(f\"Data file '{config.data_file}' not found. Cannot train tokenizer.\")\n\n    # --unk_id=0, --bos_id=1, --eos_id=2 are defaults but explicit here\n    # --pad_id=3 and --pad_piece make padding explicit\n    # --character_coverage=1.0 ensures all characters are representable\n    spm_command = (\n        f\"--input={config.data_file} --model_prefix={config.sp_model_prefix} \"\n        f\"--vocab_size={config.sp_vocab_size} --model_type={config.sp_model_type} \"\n        f\"--character_coverage=1.0 --unk_id=0 --bos_id=1 --eos_id=2 \"\n        f\"--pad_id={config.sp_pad_id} --pad_piece={config.sp_pad_piece} \"\n        f\"--unk_piece=<unk> --bos_piece=<s> --eos_piece=</s> \" # Optional: Define string representations\n        f\"--hard_vocab_limit=false \" # Allows vocab slightly larger if needed for special tokens\n        f\"--shuffle_input_sentence=true --input_sentence_size=10000000\" # Shuffle input for better training\n    )\n\n    print(f\"Running SentencePiece Trainer with command:\\n{spm_command}\\n\")\n    try:\n        spm.SentencePieceTrainer.train(spm_command)\n        print(f\"SentencePiece model trained and saved with prefix '{config.sp_model_prefix}'\")\n    except Exception as e:\n        print(f\"Error training SentencePiece: {e}\")\n        raise SystemExit(\"SentencePiece training failed.\")\n\nelse:\n    print(f\"SentencePiece model found at '{sp_model_file}'. Skipping training.\")\n\nsp_vocab_file = f\"{config.sp_model_prefix}.vocab\"\nif not os.path.exists(sp_model_file) or not os.path.exists(sp_vocab_file):\n     raise FileNotFoundError(f\"SentencePiece model/vocab files missing after training attempt.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:03.311898Z","iopub.execute_input":"2025-03-31T10:54:03.312189Z","iopub.status.idle":"2025-03-31T10:54:03.318291Z","shell.execute_reply.started":"2025-03-31T10:54:03.312168Z","shell.execute_reply":"2025-03-31T10:54:03.317446Z"}},"outputs":[{"name":"stdout","text":"SentencePiece model found at 'ts_spm_bpe.model'. Skipping training.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"\nprint(f\"Loading text corpus from: {config.data_file}\")\ntry:\n    \n    if not os.path.exists(config.data_file):\n         raise FileNotFoundError(f\"Data file '{config.data_file}' was expected but not found.\")\n\n    with open(config.data_file, 'r', encoding='utf-8') as f:\n        text_corpus = f.read()\n    print(f\"Corpus loaded. Length: {len(text_corpus):,} characters\")\n    \n\nexcept FileNotFoundError as e:\n    print(f\"ERROR: {e}\")\n    raise SystemExit(\"Data file loading failed. Stopping execution.\")\nexcept Exception as e:\n    print(f\"An error occurred while reading the file: {e}\")\n    raise SystemExit(\"File reading error. Stopping execution.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:10.351130Z","iopub.execute_input":"2025-03-31T10:54:10.351431Z","iopub.status.idle":"2025-03-31T10:54:10.358262Z","shell.execute_reply.started":"2025-03-31T10:54:10.351407Z","shell.execute_reply":"2025-03-31T10:54:10.357574Z"}},"outputs":[{"name":"stdout","text":"Loading text corpus from: /kaggle/input/taylorswift/taylorswift.txt\nCorpus loaded. Length: 185,560 characters\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"\nif 'text_corpus' in locals() and text_corpus:\n    print(\"-\" * 30)\n    print(\"Start of loaded text corpus (first 500 characters):\")\n    print(\"-\" * 30)\n    print(text_corpus[:500]) # Print the first 500 characters\n    print(\"\\n\" + \"-\" * 30)\n    print(\"[...] (rest of the corpus follows)\")\n    print(\"-\" * 30 + \"\\n\")\nelse:\n    print(\"Text corpus variable 'text_corpus' not found or empty, skipping display.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:13.371042Z","iopub.execute_input":"2025-03-31T10:54:13.371355Z","iopub.status.idle":"2025-03-31T10:54:13.377084Z","shell.execute_reply.started":"2025-03-31T10:54:13.371328Z","shell.execute_reply":"2025-03-31T10:54:13.376345Z"}},"outputs":[{"name":"stdout","text":"------------------------------\nStart of loaded text corpus (first 500 characters):\n------------------------------\nCopy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\n---\n\nMain menu\n\nWikipediaThe Free Encyclopedia\n\nSearch\nCreate account\nLog in\n\nPersonal tools\nContents  hide\n(Top)\nLife and career\nToggle Life and career subsection\nArtistry\nToggle Artistry subsection\nAccolades and achievements\nCultural status\nToggle Cultural status subsection\nWealth\nToggle Wealth subsection\nDiscography\nFilmography\nTours\nSee also\nFootnotes\nReferences\nToggle References subsection\nExternal links\nTaylor Swift\n\n\n\n------------------------------\n[...] (rest of the corpus follows)\n------------------------------\n\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"\nclass SentencePieceTokenizer:\n    def __init__(self, model_path):\n        print(f\"Loading SentencePiece model from: {model_path}\")\n        self.processor = spm.SentencePieceProcessor()\n        self.processor.load(model_path)\n\n        self._vocab_size = self.processor.get_piece_size()\n        self._pad_id = self.processor.pad_id()\n        self._unk_id = self.processor.unk_id()\n        self._bos_id = self.processor.bos_id()\n        self._eos_id = self.processor.eos_id()\n\n        print(f\"SentencePiece model loaded.\")\n        print(f\"  Vocabulary size: {self._vocab_size}\")\n        print(f\"  PAD ID: {self._pad_id} ('{self.processor.id_to_piece(self._pad_id)}')\")\n        print(f\"  UNK ID: {self._unk_id} ('{self.processor.id_to_piece(self._unk_id)}')\")\n        print(f\"  BOS ID: {self._bos_id} ('{self.processor.id_to_piece(self._bos_id)}')\")\n        print(f\"  EOS ID: {self._eos_id} ('{self.processor.id_to_piece(self._eos_id)}')\")\n\n        # Verify our configured PAD ID matches the loaded model's PAD ID\n        if self._pad_id != config.sp_pad_id:\n             print(f\"Warning: Configured PAD ID ({config.sp_pad_id}) does not match loaded model PAD ID ({self._pad_id}). Using loaded model PAD ID.\")\n             # Update config pad_id to match the actual loaded model for safety\n             config.sp_pad_id = self._pad_id\n        # --------------------\n\n\n    def encode(self, text, add_bos=False, add_eos=False):\n        \"\"\"Encodes text into token IDs.\"\"\"\n        encoded = self.processor.encode_as_ids(text)\n        if add_bos:\n            encoded = [self._bos_id] + encoded\n        if add_eos:\n            encoded = encoded + [self._eos_id]\n        return encoded\n\n    def decode(self, token_ids, skip_special_tokens=True):\n        \"\"\"Decodes token IDs back to text.\"\"\"\n        if skip_special_tokens:\n            ids_to_decode = [id for id in token_ids if id not in [self._pad_id, self._bos_id, self._eos_id, self._unk_id]]\n        else:\n            ids_to_decode = token_ids\n\n        return self.processor.decode_ids(ids_to_decode)\n\n    def vocab_size(self):\n        \"\"\"Returns the vocabulary size.\"\"\"\n        return self._vocab_size\n\n    @property\n    def pad_token_id(self):\n        return self._pad_id\n\n    @property\n    def unk_token_id(self):\n        return self._unk_id\n\n    @property\n    def bos_token_id(self):\n        return self._bos_id\n\n    @property\n    def eos_token_id(self):\n        return self._eos_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:17.436114Z","iopub.execute_input":"2025-03-31T10:54:17.436393Z","iopub.status.idle":"2025-03-31T10:54:17.444354Z","shell.execute_reply.started":"2025-03-31T10:54:17.436372Z","shell.execute_reply":"2025-03-31T10:54:17.443615Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"### 4.2 Text Dataset","metadata":{}},{"cell_type":"code","source":"\nclass TextDataset(Dataset):\n    def __init__(self, text_data, tokenizer, max_seq_len):\n        self.tokenizer = tokenizer\n        self.max_seq_len = max_seq_len # Max length in TOKENS\n        print(\"Tokenizing data using SentencePiece...\")\n        start_time = time.time()\n\n        self.tokenized_data = tokenizer.encode(text_data, add_bos=True, add_eos=True)\n        self.data_len = len(self.tokenized_data) # Length is now number of tokens\n        print(f\"Tokenization complete ({time.time() - start_time:.2f}s). Total tokens: {self.data_len:,}\")\n\n        # Sequence creation logic remains the same, but operates on tokens\n        # Example: [BOS, t1a, t1b, t2, t3a, t3b, t3c, EOS], max_seq_len=4 (tokens)\n        # Chunk 1: [BOS, t1a, t1b, t2, t3a] -> Input: [BOS, t1a, t1b, t2], Target: [t1a, t1b, t2, t3a]\n        # Chunk 2: [t1a, t1b, t2, t3a, t3b] -> Input: [t1a, t1b, t2, t3a], Target: [t1b, t2, t3a, t3b]\n        # ...\n\n    def __len__(self):\n        return max(0, self.data_len - self.max_seq_len - 1)\n\n    def __getitem__(self, idx):\n\n        start_idx = idx\n        end_idx = idx + self.max_seq_len + 1\n        chunk = self.tokenized_data[start_idx : end_idx]\n        current_chunk_len = len(chunk)\n        if current_chunk_len < self.max_seq_len + 1:\n             print(f\"Warning: Chunk at index {idx} is shorter than expected ({current_chunk_len} vs {self.max_seq_len + 1}). Padding.\")\n             padding_needed = (self.max_seq_len + 1) - current_chunk_len\n             # Pad with PAD token ID\n             chunk.extend([self.tokenizer.pad_token_id] * padding_needed)\n\n\n        input_ids = torch.tensor(chunk[:-1], dtype=torch.long)\n        target_ids = torch.tensor(chunk[1:], dtype=torch.long)\n\n        # Double-check lengths after potential padding or slicing issues\n        if len(input_ids) != self.max_seq_len:\n             raise RuntimeError(f\"Input length mismatch after processing: expected {self.max_seq_len}, got {len(input_ids)} at index {idx}\")\n        if len(target_ids) != self.max_seq_len:\n             raise RuntimeError(f\"Target length mismatch after processing: expected {self.max_seq_len}, got {len(target_ids)} at index {idx}\")\n\n\n        return input_ids, target_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:21.195477Z","iopub.execute_input":"2025-03-31T10:54:21.195816Z","iopub.status.idle":"2025-03-31T10:54:21.202435Z","shell.execute_reply.started":"2025-03-31T10:54:21.195787Z","shell.execute_reply":"2025-03-31T10:54:21.201586Z"}},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":"### 4.6 Instantiate Tokenizer, Dataset, and DataLoader","metadata":{}},{"cell_type":"code","source":"# --- Create SentencePiece Tokenizer ---\n# Load the model trained/verified in the previous step\nsp_model_path = f\"{config.sp_model_prefix}.model\"\ntry:\n    # Ensure the text_corpus variable exists from the loading step\n    if 'text_corpus' not in locals() or not text_corpus:\n         raise RuntimeError(\"Text corpus not loaded. Cannot proceed.\")\n    tokenizer = SentencePieceTokenizer(sp_model_path)\nexcept Exception as e:\n     print(f\"Error loading SentencePiece model from '{sp_model_path}': {e}\")\n     print(\"Ensure the SentencePiece training cell ran successfully and the .model file exists.\")\n     raise SystemExit(\"Tokenizer loading failed.\")\n\n# Update config with the ACTUAL vocabulary size from the loaded SentencePiece model\nconfig.vocab_size = tokenizer.vocab_size()\nprint(f\"Updated config.vocab_size to actual SentencePiece vocab size: {config.vocab_size}\")\n\n\n# --- Create Dataset ---\ntry:\n    dataset = TextDataset(text_corpus, tokenizer, config.max_seq_len)\nexcept Exception as e:\n    print(f\"Error creating TextDataset: {e}\")\n    raise SystemExit(\"Dataset creation failed.\")\n\n\n# --- Create DataLoader ---\n# Use multiple workers for data loading if on GPU and OS supports it well\nnum_workers = 2 if config.device.type == 'cuda' and os.name == 'posix' else 0 # Use workers mainly on Linux/macOS with CUDA\ndataloader = DataLoader(\n    dataset,\n    batch_size=config.batch_size,\n    shuffle=True, # Shuffle data each epoch for better training\n    num_workers=num_workers,\n    pin_memory=(config.device.type == 'cuda') # Helps speed up CPU->GPU transfer\n)\n\nprint(f\"\\nDataset size: {len(dataset):,} sequences\")\nprint(f\"Final Vocab size used in model: {config.vocab_size}\")\n\n# --- Print Example to Verify ---\nif len(dataset) > 0:\n    print(\"\\n--- Verifying Dataloader Output ---\")\n    try:\n        example_batch_input, example_batch_target = next(iter(dataloader))\n        print(f\"Sample batch shapes: input={example_batch_input.shape}, target={example_batch_target.shape}\")\n\n        # Take the first item from the batch for detailed view\n        example_input = example_batch_input[0]\n        example_target = example_batch_target[0]\n\n        print(f\"\\nExample item from first batch:\")\n        print(f\"Input tokens : {example_input[:30].tolist()}...\")\n        print(f\"Target tokens: {example_target[:30].tolist()}...\")\n        print(\"-\" * 20)\n        # Decode example tokens back to text, skipping special tokens for readability\n        decoded_input_sample = tokenizer.decode(example_input[:50].tolist(), skip_special_tokens=True)\n        decoded_target_sample = tokenizer.decode(example_target[:50].tolist(), skip_special_tokens=True)\n        print(f\"Decoded input sample : '{decoded_input_sample}'...\")\n        print(f\"Decoded target sample: '{decoded_target_sample}'...\")\n        print(\"-\" * 20)\n    except StopIteration:\n        print(\"Could not get a batch from the dataloader (dataset might be smaller than batch size).\")\n    except Exception as e:\n        print(f\"Error verifying dataloader output: {e}\")\nelse:\n    print(\"\\nWarning: Dataset is empty. Cannot verify dataloader. Check data file, tokenization, and max_seq_len.\")\n    if 'text_corpus' in locals() and len(text_corpus) > 0:\n         print(f\"Text corpus length ({len(text_corpus)} chars) resulted in {len(dataset)} sequences of {config.max_seq_len} tokens.\")\n\n# Clean up large text variable if no longer needed\nif 'text_corpus' in locals():\n    del text_corpus\n    print(\"Cleaned up text_corpus variable from memory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:26.431181Z","iopub.execute_input":"2025-03-31T10:54:26.431474Z","iopub.status.idle":"2025-03-31T10:54:26.666532Z","shell.execute_reply.started":"2025-03-31T10:54:26.431451Z","shell.execute_reply":"2025-03-31T10:54:26.665701Z"}},"outputs":[{"name":"stdout","text":"Loading SentencePiece model from: ts_spm_bpe.model\nSentencePiece model loaded.\n  Vocabulary size: 4000\n  PAD ID: 3 ('<pad>')\n  UNK ID: 0 ('<unk>')\n  BOS ID: 1 ('<s>')\n  EOS ID: 2 ('</s>')\nUpdated config.vocab_size to actual SentencePiece vocab size: 4000\nTokenizing data using SentencePiece...\nTokenization complete (0.13s). Total tokens: 43,533\n\nDataset size: 43,404 sequences\nFinal Vocab size used in model: 4000\n\n--- Verifying Dataloader Output ---\nSample batch shapes: input=torch.Size([32, 128]), target=torch.Size([32, 128])\n\nExample item from first batch:\nInput tokens : [3770, 1418, 472, 79, 112, 32, 3942, 3916, 1173, 1417, 38, 626, 495, 3158, 81, 3391, 149, 3921, 78, 66, 17, 76, 48, 269, 434, 160, 54, 190, 397, 195]...\nTarget tokens: [1418, 472, 79, 112, 32, 3942, 3916, 1173, 1417, 38, 626, 495, 3158, 81, 3391, 149, 3921, 78, 66, 17, 76, 48, 269, 434, 160, 54, 190, 397, 195, 984]...\n--------------------\nDecoded input sample : 'Here Are All of Taylor Swift's Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022. Trust, Gary (June 5, 2023). \"Morgan Wallen's 'Last Night' No.'...\nDecoded target sample: 'Are All of Taylor Swift's Biggest Accomplishments in 2022\". Billboard. Archived from the original on April 30, 2023. Retrieved December 7, 2022. Trust, Gary (June 5, 2023). \"Morgan Wallen's 'Last Night' No. 1'...\n--------------------\nCleaned up text_corpus variable from memory.\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"### Relative Position Bias","metadata":{}},{"cell_type":"code","source":"class RelativePositionBias(nn.Module):\n    \"\"\"\n    Simplified Relative Position Bias module, inspired by T5.\n    Learns embeddings for relative distances between keys and queries.\n    Each head gets its own set of biases.\n    \"\"\"\n    def __init__(self, num_buckets, num_heads, max_distance=128, bidirectional=True):\n        super().__init__()\n        self.num_buckets = num_buckets\n        self.num_heads = num_heads\n        self.max_distance = max_distance\n        self.bidirectional = bidirectional\n        # Learnable embeddings: one vector of size num_heads for each bucket index\n        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.num_heads)\n        # Initialize biases to zeros (optional, but common)\n        nn.init.zeros_(self.relative_attention_bias.weight)\n\n\n    def _relative_position_bucket(self, relative_position):\n        \"\"\" Maps relative position values to integer bucket indices. \"\"\"\n        ret = 0\n        n = -relative_position # Positive values mean query is ahead of key\n        num_buckets = self.num_buckets\n        max_dist = self.max_distance\n\n        if self.bidirectional:\n            # Split buckets for positive and negative positions\n            num_buckets //= 2\n            # Add offset for negative positions (if n < 0)\n            ret += (n < 0).to(torch.long) * num_buckets\n            n = torch.abs(n) # Consider absolute distance now\n        else:\n            # For unidirectional (decoder self-attention), clip non-positive positions to 0\n            n = torch.max(n, torch.zeros_like(n))\n\n        # Now n is non-negative\n        # Bucketing logic: linear up to max_exact, logarithmic beyond\n        max_exact = num_buckets // 2\n        is_small = (n < max_exact)\n\n        # Calculate bucket index for large distances logarithmically\n        # Add epsilon for numerical stability with log\n        # Clamp log argument to avoid log(0)\n        val_if_large = max_exact + (\n            torch.log(n.float() / max_exact + 1e-6)\n            / math.log(max_dist / max_exact + 1e-6) # Add epsilon here too\n            * (num_buckets - max_exact)\n        ).to(torch.long)\n\n        # Ensure bucket indices stay within bounds [0, num_buckets-1]\n        val_if_large = torch.min(val_if_large, torch.full_like(n, num_buckets - 1))\n        # Also ensure indices are non-negative if calculation goes wrong\n        val_if_large = torch.max(val_if_large, torch.zeros_like(n))\n\n\n        # Combine buckets for small and large distances based on the `is_small` mask\n        ret += torch.where(is_small, n, val_if_large)\n        # Final check to ensure all bucket indices are within the valid range\n        ret = torch.clamp(ret, 0, self.num_buckets - 1 if self.bidirectional else self.num_buckets // 2 * 2 -1 )\n\n        return ret\n\n    def forward(self, qlen, klen, device):\n        \"\"\" Calculate relative position bias matrix for given sequence lengths. \"\"\"\n        # Generate tensors representing positions in the query and key sequences\n        context_position = torch.arange(qlen, dtype=torch.long, device=device)[:, None] # Shape (qlen, 1)\n        memory_position = torch.arange(klen, dtype=torch.long, device=device)[None, :]  # Shape (1, klen)\n\n        # Calculate pairwise relative positions: shape (qlen, klen)\n        # E.g., result[i, j] = memory_position[j] - context_position[i]\n        relative_position = memory_position - context_position\n\n        # Compute bucket indices for each relative position\n        rp_bucket = self._relative_position_bucket(relative_position) # Shape (qlen, klen)\n\n        # Look up the learned bias vectors from the embedding table using the bucket indices\n        # Shape: (qlen, klen, num_heads)\n        values = self.relative_attention_bias(rp_bucket)\n\n        # Reshape for broadcasting with attention scores: (num_heads, qlen, klen)\n        values = values.permute(2, 0, 1)\n        return values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:31.031453Z","iopub.execute_input":"2025-03-31T10:54:31.031812Z","iopub.status.idle":"2025-03-31T10:54:31.040375Z","shell.execute_reply.started":"2025-03-31T10:54:31.031781Z","shell.execute_reply":"2025-03-31T10:54:31.039454Z"}},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":"### mutli head attention","metadata":{}},{"cell_type":"code","source":"\nclass MultiHeadAttention(nn.Module):\n    \"\"\" Multi-Head Attention Layer incorporating Relative Position Bias. \"\"\"\n    def __init__(self, d_model, nhead, dropout=0.1, is_decoder=False, relative_attention_num_buckets=32):\n        super().__init__()\n        assert d_model % nhead == 0, \"d_model must be divisible by nhead\"\n        self.d_model = d_model\n        self.nhead = nhead\n        self.d_k = d_model // nhead # Dimension of each attention head\n        self.is_decoder = is_decoder # Flag for relative bias directionality\n\n        # Linear projections for Query, Key, Value, and Output. Bias is often False.\n        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n        self.out_proj = nn.Linear(d_model, d_model, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n        # Instantiate the Relative Position Bias module\n        self.relative_position_bias = RelativePositionBias(\n            num_buckets=relative_attention_num_buckets,\n            num_heads=nhead,\n            bidirectional=(not is_decoder) # Encoder is bidirectional, Decoder self-attn is unidirectional\n        )\n\n    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None):\n        \"\"\"\n        Args:\n            query (Tensor): Query tensor, shape (batch_size, target_len, d_model)\n            key (Tensor): Key tensor, shape (batch_size, source_len, d_model)\n            value (Tensor): Value tensor, shape (batch_size, source_len, d_model)\n            key_padding_mask (BoolTensor, optional): Mask for padding tokens in key/value. Shape (batch_size, source_len). True where padded.\n            attn_mask (Tensor, optional): Mask to prevent attention to certain positions (e.g., future tokens). Shape (target_len, source_len) or broadcastable. Can be bool (True where masked) or float (-inf where masked).\n\n        Returns:\n            Tensor: Output tensor, shape (batch_size, target_len, d_model)\n        \"\"\"\n        batch_size = query.size(0)\n        target_len = query.size(1)\n        source_len = key.size(1)\n        device = query.device\n\n        # 1. Linear projections: (batch_size, seq_len, d_model)\n        q = self.q_proj(query)\n        k = self.k_proj(key)\n        v = self.v_proj(value)\n\n        # 2. Reshape for multi-head attention:\n        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, nhead, d_k) -> (batch_size, nhead, seq_len, d_k)\n        q = q.view(batch_size, target_len, self.nhead, self.d_k).transpose(1, 2)\n        k = k.view(batch_size, source_len, self.nhead, self.d_k).transpose(1, 2)\n        v = v.view(batch_size, source_len, self.nhead, self.d_k).transpose(1, 2)\n\n        # 3. Scaled dot-product attention scores:\n        # (batch_size, nhead, target_len, d_k) @ (batch_size, nhead, d_k, source_len) -> (batch_size, nhead, target_len, source_len)\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        # 4. Add Relative Position Bias:\n        # Compute bias matrix: (nhead, target_len, source_len)\n        rel_pos_bias = self.relative_position_bias(target_len, source_len, device=device)\n        # Add bias (broadcasts across batch dim): -> (batch_size, nhead, target_len, source_len)\n        attn_scores = attn_scores + rel_pos_bias.unsqueeze(0)\n\n        # 5. Apply masks:\n        # Apply attn_mask (e.g., causal mask)\n        if attn_mask is not None:\n             # Ensure attn_mask is broadcastable: e.g., (target_len, source_len) -> (1, 1, target_len, source_len)\n             if attn_mask.dim() == 2:\n                 attn_mask = attn_mask.unsqueeze(0).unsqueeze(0)\n             elif attn_mask.dim() == 3: # Should not happen for standard masks\n                 attn_mask = attn_mask.unsqueeze(1)\n\n             # Apply mask where mask is True (bool) or non-zero/negative (float)\n             if attn_mask.dtype == torch.bool:\n                # Ensure mask is on the same device and has correct shape for broadcasting\n                attn_scores = attn_scores.masked_fill(attn_mask.to(device), float('-inf'))\n             else: # Assuming float mask with large negative values\n                attn_scores = attn_scores + attn_mask.to(device) # Broadcasting\n\n        # Apply key_padding_mask (masks padding tokens in K/V)\n        if key_padding_mask is not None:\n            # Reshape mask for broadcasting: (batch_size, source_len) -> (batch_size, 1, 1, source_len)\n            mask = key_padding_mask.unsqueeze(1).unsqueeze(2).to(device)\n            attn_scores = attn_scores.masked_fill(mask == True, float('-inf'))\n\n        # 6. Softmax to get attention probabilities:\n        attn_probs = F.softmax(attn_scores, dim=-1) # Apply softmax over source length dimension\n        attn_probs = self.dropout(attn_probs) # Apply dropout to attention weights\n\n        # 7. Weighted sum of values:\n        # (batch_size, nhead, target_len, source_len) @ (batch_size, nhead, source_len, d_k) -> (batch_size, nhead, target_len, d_k)\n        output = torch.matmul(attn_probs, v)\n\n        # 8. Reshape and final linear projection:\n        # (batch_size, nhead, target_len, d_k) -> (batch_size, target_len, nhead, d_k) -> (batch_size, target_len, d_model)\n        output = output.transpose(1, 2).contiguous().view(batch_size, target_len, self.d_model)\n        output = self.out_proj(output) # Final linear layer\n\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:34.031631Z","iopub.execute_input":"2025-03-31T10:54:34.031973Z","iopub.status.idle":"2025-03-31T10:54:34.042645Z","shell.execute_reply.started":"2025-03-31T10:54:34.031946Z","shell.execute_reply":"2025-03-31T10:54:34.041788Z"}},"outputs":[],"execution_count":88},{"cell_type":"markdown","source":"### Position-wise Feed-Forward Network","metadata":{}},{"cell_type":"code","source":"\nclass PositionwiseFeedForward(nn.Module):\n    \"\"\" Implements the Position-wise Feed-Forward layer of a Transformer. \"\"\"\n    def __init__(self, d_model, dim_feedforward, dropout=0.1):\n        super().__init__()\n        # Two linear layers with an activation function and dropout in between\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        # Activation: ReLU is simple, GELU is common in modern Transformers\n        self.activation = nn.ReLU()\n        # self.activation = nn.GELU() # Option: Use GELU\n\n    def forward(self, x):\n        \"\"\" Applies the FFN transformation element-wise. \"\"\"\n        # x shape: (batch_size, seq_len, d_model)\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.dropout(x) # Dropout often applied after activation\n        x = self.linear2(x)\n        # Output shape: (batch_size, seq_len, d_model)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:37.771196Z","iopub.execute_input":"2025-03-31T10:54:37.771517Z","iopub.status.idle":"2025-03-31T10:54:37.776296Z","shell.execute_reply.started":"2025-03-31T10:54:37.771491Z","shell.execute_reply":"2025-03-31T10:54:37.775452Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### encoder layer","metadata":{}},{"cell_type":"code","source":"\nclass EncoderLayer(nn.Module):\n    \"\"\" Single Encoder layer combining Self-Attention and FFN. \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets):\n        super().__init__()\n        # Sub-layers: Multi-Head Self-Attention and Position-wise Feed-Forward\n        self.self_attn = MultiHeadAttention(d_model, nhead, dropout, is_decoder=False, relative_attention_num_buckets=relative_attention_num_buckets)\n        self.feed_forward = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n\n        # Layer Normalization and Dropout for residual connections\n        self.norm1 = nn.LayerNorm(d_model) # T5 might use RMSNorm instead\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        \"\"\" Forward pass for the encoder layer. \"\"\"\n        # src shape: (batch_size, src_seq_len, d_model)\n\n        # --- Self-attention block (Sublayer 1) ---\n        # Calculate attention output\n        attn_output = self.self_attn(src, src, src, # Query, Key, Value are all 'src'\n                                      key_padding_mask=src_key_padding_mask,\n                                      attn_mask=src_mask) # src_mask is rarely used unless specific masking needed\n        # Residual connection + Dropout + LayerNorm (Post-LN style)\n        src = src + self.dropout1(attn_output)\n        src = self.norm1(src)\n        # --- End Self-attention ---\n\n        # --- Feed-forward block (Sublayer 2) ---\n        # Pass through FFN\n        ff_output = self.feed_forward(src)\n        # Residual connection + Dropout + LayerNorm (Post-LN style)\n        src = src + self.dropout2(ff_output)\n        src = self.norm2(src)\n        # --- End Feed-forward ---\n\n        # Output shape: (batch_size, src_seq_len, d_model)\n        return src","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:41.651580Z","iopub.execute_input":"2025-03-31T10:54:41.651928Z","iopub.status.idle":"2025-03-31T10:54:41.657800Z","shell.execute_reply.started":"2025-03-31T10:54:41.651899Z","shell.execute_reply":"2025-03-31T10:54:41.656749Z"}},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":"### decoder layer","metadata":{}},{"cell_type":"code","source":"\nclass DecoderLayer(nn.Module):\n    \"\"\" Single Decoder layer combining Masked Self-Attention, Cross-Attention, and FFN. \"\"\"\n    def __init__(self, d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets):\n        super().__init__()\n        # Sub-layers: Masked Self-Attention, Cross-Attention, Position-wise Feed-Forward\n        self.self_attn = MultiHeadAttention(d_model, nhead, dropout, is_decoder=True, relative_attention_num_buckets=relative_attention_num_buckets)\n        self.cross_attn = MultiHeadAttention(d_model, nhead, dropout, is_decoder=True, relative_attention_num_buckets=relative_attention_num_buckets)\n        self.feed_forward = PositionwiseFeedForward(d_model, dim_feedforward, dropout)\n\n        # Layer Normalization (one after each sublayer) and Dropout\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.dropout3 = nn.Dropout(dropout)\n\n    def forward(self, tgt, memory,\n                tgt_mask=None, memory_mask=None,\n                tgt_key_padding_mask=None, # <--- ADDED THIS ARGUMENT\n                memory_key_padding_mask=None,\n                return_attention_weights=False):\n        \"\"\" Forward pass for the decoder layer. \"\"\"\n        # tgt shape: (batch_size, tgt_seq_len, d_model)\n        # memory shape: (batch_size, src_seq_len, d_model) - Output from Encoder\n\n        # --- Masked Self-attention block (Sublayer 1) ---\n        # Attends to the decoder input sequence (`tgt`) itself, using causal mask.\n        self_attn_output = self.self_attn(tgt, tgt, tgt, # Q, K, V = tgt\n                                           key_padding_mask=tgt_key_padding_mask, # Mask padding in tgt\n                                           attn_mask=tgt_mask) # Apply causal mask\n        # Residual connection + Dropout + LayerNorm\n        tgt = tgt + self.dropout1(self_attn_output)\n        tgt = self.norm1(tgt)\n        # --- End Masked Self-attention ---\n\n        # --- Cross-attention block (Sublayer 2) ---\n        # Attends to the encoder output (`memory`) using the decoder state (`tgt`) as query.\n        cross_attn_output = self.cross_attn(tgt, memory, memory, # Query=tgt, Key=memory, Value=memory\n                                             key_padding_mask=memory_key_padding_mask, # Mask padding in memory\n                                             attn_mask=memory_mask) # Usually None\n        # Residual connection + Dropout + LayerNorm\n        tgt = tgt + self.dropout2(cross_attn_output)\n        tgt = self.norm2(tgt)\n        # --- End Cross-attention ---\n\n        # --- Feed-forward block (Sublayer 3) ---\n        ff_output = self.feed_forward(tgt)\n        # Residual connection + Dropout + LayerNorm\n        tgt = tgt + self.dropout3(ff_output)\n        tgt = self.norm3(tgt)\n        # --- End Feed-forward ---\n\n        # Output shape: (batch_size, tgt_seq_len, d_model)\n        return tgt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:44.235884Z","iopub.execute_input":"2025-03-31T10:54:44.236174Z","iopub.status.idle":"2025-03-31T10:54:44.242647Z","shell.execute_reply.started":"2025-03-31T10:54:44.236153Z","shell.execute_reply":"2025-03-31T10:54:44.241783Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \"\"\" Stack of EncoderLayers. \"\"\"\n    def __init__(self, encoder_layer_config, num_layers, d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets, norm=True):\n        super().__init__()\n        # Create a list of independent EncoderLayer instances\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets)\n            for _ in range(num_layers)\n        ])\n        self.num_layers = num_layers\n        # Optional final Layer Normalization after the stack\n        self.norm = nn.LayerNorm(d_model) if norm else None\n\n    def forward(self, src, mask=None, src_key_padding_mask=None):\n        \"\"\" Pass input through the stack of encoder layers. \"\"\"\n        output = src\n        # Sequentially apply each layer\n        for layer in self.layers:\n            output = layer(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n\n        # Apply final normalization if specified\n        if self.norm is not None:\n            output = self.norm(output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:46.651121Z","iopub.execute_input":"2025-03-31T10:54:46.651450Z","iopub.status.idle":"2025-03-31T10:54:46.656843Z","shell.execute_reply.started":"2025-03-31T10:54:46.651420Z","shell.execute_reply":"2025-03-31T10:54:46.655895Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"\nclass Decoder(nn.Module):\n    \"\"\" Stack of DecoderLayers. \"\"\"\n    def __init__(self, decoder_layer_config, num_layers, d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets, norm=True):\n        super().__init__()\n         # Create a list of independent DecoderLayer instances\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, nhead, dim_feedforward, dropout, relative_attention_num_buckets)\n            for _ in range(num_layers)\n        ])\n        self.num_layers = num_layers\n        # Optional final Layer Normalization after the stack\n        self.norm = nn.LayerNorm(d_model) if norm else None\n\n    def forward(self, tgt, memory,\n                tgt_mask=None, memory_mask=None,\n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        \"\"\" Pass input and memory through the stack of decoder layers. \"\"\"\n        output = tgt\n        # Sequentially apply each layer, passing memory and all masks\n        for layer in self.layers:\n            output = layer(output, memory,\n                           tgt_mask=tgt_mask, memory_mask=memory_mask,\n                           tgt_key_padding_mask=tgt_key_padding_mask,\n                           memory_key_padding_mask=memory_key_padding_mask)\n\n        # Apply final normalization if specified\n        if self.norm is not None:\n            output = self.norm(output)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:48.971296Z","iopub.execute_input":"2025-03-31T10:54:48.971596Z","iopub.status.idle":"2025-03-31T10:54:48.977409Z","shell.execute_reply.started":"2025-03-31T10:54:48.971572Z","shell.execute_reply":"2025-03-31T10:54:48.976350Z"}},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":"### T5 Model (Putting it all together)","metadata":{}},{"cell_type":"code","source":"\nclass T5Model(nn.Module):\n    \"\"\" The main T5 model combining Encoder, Decoder, and Shared Embeddings. \"\"\"\n    def __init__(self, config: Config, tokenizer: SentencePieceTokenizer): # Pass tokenizer to get pad_id\n        super().__init__()\n        self.config = config\n        self.d_model = config.d_model\n        # Get vocab size from config (set after tokenizer loaded)\n        self.vocab_size = config.vocab_size\n        # Get PAD ID directly from the tokenizer instance\n        self.pad_token_id = tokenizer.pad_token_id\n\n        if self.vocab_size is None or self.pad_token_id is None:\n             raise ValueError(\"Vocab size or Pad token ID is not set in config/tokenizer.\")\n\n        print(f\"Initializing T5Model with vocab_size={self.vocab_size} and pad_token_id={self.pad_token_id}\")\n\n        # --- Shared Embedding Layer ---\n        # Uses the actual vocab size and the correct padding index from the tokenizer\n        self.shared_embedding = nn.Embedding(self.vocab_size, config.d_model, padding_idx=self.pad_token_id)\n        # Scale embeddings (common practice, helps stabilize)\n        self.scale_emb = math.sqrt(self.d_model)\n\n        # --- Dropout for embeddings ---\n        self.dropout = nn.Dropout(config.dropout)\n\n        # --- Encoder ---\n        # Pass config values directly to the Encoder constructor\n        encoder_norm = True # T5 typically uses a final norm in the encoder\n        self.encoder = Encoder(\n            encoder_layer_config={}, # Config passed directly below\n            num_layers=config.num_encoder_layers,\n            d_model=config.d_model,\n            nhead=config.nhead,\n            dim_feedforward=config.dim_feedforward,\n            dropout=config.dropout,\n            relative_attention_num_buckets=config.relative_attention_num_buckets,\n            norm=encoder_norm\n        )\n\n        # --- Decoder ---\n        decoder_norm = True # T5 typically uses a final norm in the decoder\n        self.decoder = Decoder(\n            decoder_layer_config={}, # Config passed directly below\n            num_layers=config.num_decoder_layers,\n            d_model=config.d_model,\n            nhead=config.nhead,\n            dim_feedforward=config.dim_feedforward,\n            dropout=config.dropout,\n            relative_attention_num_buckets=config.relative_attention_num_buckets,\n            norm=decoder_norm\n        )\n\n        # --- Final Linear Layer (Output Head) ---\n        # Projects decoder output (d_model) to vocabulary scores (vocab_size)\n        self.lm_head = nn.Linear(config.d_model, self.vocab_size, bias=False)\n\n        # --- Weight Tying (Crucial T5 Feature) ---\n        # Share weights between input embeddings and final output layer\n        self.lm_head.weight = self.shared_embedding.weight\n        self._init_weights()\n        print(\"T5 Model components initialized.\")\n\n    def _init_weights(self):\n        \"\"\" Initializes model weights. \"\"\"\n        # Simple initialization scheme. More sophisticated methods exist (e.g., T5 uses truncated normal).\n        initrange = 0.1\n        # Initialize shared embeddings uniformly\n        self.shared_embedding.weight.data.uniform_(-initrange, initrange)\n        # lm_head weights are automatically initialized due to tying\n\n        # Initialize linear layers (e.g., in Attention, FFN) using Xavier/Glorot Uniform\n        # Initialize LayerNorm weights to 1, biases to 0\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                 nn.init.xavier_uniform_(module.weight)\n                 if module.bias is not None: # Check if bias exists (we set bias=False in some layers)\n                     nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.LayerNorm):\n                 nn.init.ones_(module.weight)\n                 nn.init.zeros_(module.bias)\n        print(\"Model weights initialized (Embeddings: Uniform, Linear: Xavier, LayerNorm: Default).\")\n\n\n    def forward(self, src_ids, tgt_ids, src_padding_mask=None, tgt_padding_mask=None, memory_key_padding_mask=None, tgt_mask=None):\n        \"\"\"\n        Performs the forward pass of the T5 model.\n        Args:\n            src_ids (Tensor): Input token IDs for the encoder. Shape (batch_size, src_seq_len)\n            tgt_ids (Tensor): Input token IDs for the decoder (shifted right). Shape (batch_size, tgt_seq_len)\n            ... masks ...\n        Returns:\n            Tensor: Logits output from the decoder head. Shape (batch_size, tgt_seq_len, vocab_size)\n        \"\"\"\n        # 1. Embeddings (Source and Target use the same embedding layer)\n        src_emb = self.shared_embedding(src_ids) * self.scale_emb\n        tgt_emb = self.shared_embedding(tgt_ids) * self.scale_emb\n\n        # Apply dropout to embeddings\n        src_emb = self.dropout(src_emb)\n        tgt_emb = self.dropout(tgt_emb)\n\n        # Note: Relative position bias is handled within the attention layers.\n\n        # 2. Encoder Pass\n        # Encodes the source sequence.\n        # memory shape: (batch_size, src_seq_len, d_model)\n        memory = self.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n\n        # 3. Decoder Pass\n        # Takes encoder output (memory) and the shifted target sequence embeddings.\n        # decoder_output shape: (batch_size, tgt_seq_len, d_model)\n        decoder_output = self.decoder(tgt_emb, memory,\n                                      tgt_mask=tgt_mask,                # Causal mask for decoder self-attention\n                                      tgt_key_padding_mask=tgt_padding_mask, # Padding mask for decoder input tokens\n                                      memory_key_padding_mask=memory_key_padding_mask) # Padding mask for encoder output (memory)\n\n        # 4. Final Linear Layer (Logits)\n        # Project decoder output to vocabulary space.\n        # logits shape: (batch_size, tgt_seq_len, vocab_size)\n        logits = self.lm_head(decoder_output)\n\n        return logits\n\n    # Helper function to create all necessary masks based on input token IDs\n    def create_masks(self, src_ids, tgt_ids):\n        \"\"\"\n        Generates padding masks and the target causal mask.\n        Uses the model's internal pad_token_id.\n        \"\"\"\n        device = src_ids.device\n        # Source padding mask: True where src_ids is padding (ID = self.pad_token_id)\n        src_padding_mask = (src_ids == self.pad_token_id) # (batch_size, src_seq_len)\n\n        # Target padding mask: True where tgt_ids (decoder input) is padding\n        tgt_padding_mask = (tgt_ids == self.pad_token_id) # (batch_size, tgt_seq_len)\n\n        # Memory key padding mask (for cross-attention): Based on source padding.\n        # True where the *memory* comes from a padded source token.\n        memory_key_padding_mask = src_padding_mask # (batch_size, src_seq_len)\n\n        # Target causal (look-ahead) mask: Prevents attention to future tokens in the target sequence.\n        tgt_seq_len = tgt_ids.size(1)\n        # Generates a square matrix where the upper triangle (future positions) is masked.\n        # PyTorch's function returns float tensor with -inf for masked positions.\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len, device=device)\n        # Shape: (tgt_seq_len, tgt_seq_len)\n\n        return src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:50.811586Z","iopub.execute_input":"2025-03-31T10:54:50.811927Z","iopub.status.idle":"2025-03-31T10:54:50.823114Z","shell.execute_reply.started":"2025-03-31T10:54:50.811899Z","shell.execute_reply":"2025-03-31T10:54:50.822105Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"\nif config.vocab_size is None or 'tokenizer' not in locals() or tokenizer is None:\n    raise ValueError(\"Config vocab_size not set or tokenizer not loaded. Run data loading cells first.\")\n\nprint(f\"Initializing model with vocab size: {config.vocab_size}\")\nmodel = T5Model(config, tokenizer).to(config.device) # Pass tokenizer to get pad_id\n\n# Print model parameter count for verification\nnum_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel created on {config.device}.\")\nprint(f\"Total Trainable Parameters: {num_params:,}\")\n\n# Optional: Print model structure summary (can be very long)\nprint(\"\\nModel Structure:\")\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:48:00.167916Z","iopub.execute_input":"2025-03-31T10:48:00.168210Z","iopub.status.idle":"2025-03-31T10:48:00.929042Z","shell.execute_reply.started":"2025-03-31T10:48:00.168186Z","shell.execute_reply":"2025-03-31T10:48:00.928209Z"}},"outputs":[{"name":"stdout","text":"Initializing model with vocab size: 4000\nInitializing T5Model with vocab_size=4000 and pad_token_id=3\nModel weights initialized (Embeddings: Uniform, Linear: Xavier, LayerNorm: Default).\nT5 Model components initialized.\n\nModel created on cuda.\nTotal Trainable Parameters: 46,156,288\n\nModel Structure:\nT5Model(\n  (shared_embedding): Embedding(4000, 512, padding_idx=3)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (encoder): Encoder(\n    (layers): ModuleList(\n      (0-5): 6 x EncoderLayer(\n        (self_attn): MultiHeadAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (relative_position_bias): RelativePositionBias(\n            (relative_attention_bias): Embedding(32, 8)\n          )\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (decoder): Decoder(\n    (layers): ModuleList(\n      (0-5): 6 x DecoderLayer(\n        (self_attn): MultiHeadAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (relative_position_bias): RelativePositionBias(\n            (relative_attention_bias): Embedding(32, 8)\n          )\n        )\n        (cross_attn): MultiHeadAttention(\n          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n          (out_proj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (relative_position_bias): RelativePositionBias(\n            (relative_attention_bias): Embedding(32, 8)\n          )\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n          (activation): ReLU()\n        )\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n        (dropout3): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=512, out_features=4000, bias=False)\n)\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0.01)\nprint(f\"Optimizer: AdamW (lr={config.learning_rate}, weight_decay=0.01)\")\n\n# Loss Function: Cross Entropy Loss for classification (predicting next token)\n# Crucially, ignore the padding token ID during loss calculation.\ncriterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id) # Use pad_id from tokenizer\nprint(f\"Loss Function: CrossEntropyLoss (ignoring padding token ID: {tokenizer.pad_token_id})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:54:54.701457Z","iopub.execute_input":"2025-03-31T10:54:54.701785Z","iopub.status.idle":"2025-03-31T10:54:54.708041Z","shell.execute_reply.started":"2025-03-31T10:54:54.701759Z","shell.execute_reply":"2025-03-31T10:54:54.707048Z"}},"outputs":[{"name":"stdout","text":"Optimizer: AdamW (lr=0.0001, weight_decay=0.01)\nLoss Function: CrossEntropyLoss (ignoring padding token ID: 3)\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"# %% [markdown]\n# ### 7.1 Training Epoch Function\n# Defines the function to run one epoch of training.\n\n# %%\ndef train_epoch(model, dataloader, optimizer, criterion, config, epoch_num):\n    \"\"\"Runs one training epoch.\"\"\"\n    model.train() # Set model to training mode (enables dropout, etc.)\n    total_loss = 0.0\n    num_batches = len(dataloader)\n\n    # Wrap dataloader with tqdm for a progress bar\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch_num+1}/{config.epochs} Training\", leave=False, unit=\"batch\")\n\n    for batch_idx, (input_ids, target_ids) in enumerate(progress_bar):\n        # Move data to the configured device\n        # input_ids = context, target_ids = sequence shifted left by one\n        input_ids = input_ids.to(config.device)   # Shape: (batch_size, max_seq_len)\n        target_ids = target_ids.to(config.device) # Shape: (batch_size, max_seq_len)\n\n        # --- Prepare inputs for T5 LM ---\n        # Encoder input = `input_ids`\n        # Decoder input = `input_ids` (acts as the shifted-right input for prediction)\n        # Target for loss = `target_ids` (the actual next tokens)\n\n        # Create masks based on the *input* sequences using the model's helper method\n        # Note: We pass input_ids for both src and tgt mask generation in this LM setup\n        src_padding_mask, tgt_padding_mask, memory_key_padding_mask, tgt_mask = \\\n            model.create_masks(src_ids=input_ids, tgt_ids=input_ids)\n\n        # --- Training Step ---\n        optimizer.zero_grad() # Reset gradients from previous batch\n\n        # Forward pass: Get model predictions (logits)\n        logits = model(src_ids=input_ids, tgt_ids=input_ids, # Use same sequence for enc/dec input\n                       src_padding_mask=src_padding_mask,\n                       tgt_padding_mask=tgt_padding_mask, # Mask padding in decoder input\n                       memory_key_padding_mask=memory_key_padding_mask, # Mask padding in encoder output\n                       tgt_mask=tgt_mask) # Apply causal mask in decoder\n        # Logits shape: (batch_size, max_seq_len, vocab_size)\n\n        # Calculate loss\n        # Reshape logits to (N, C) and targets to (N) for CrossEntropyLoss\n        loss = criterion(logits.view(-1, config.vocab_size), target_ids.view(-1))\n\n        # Backward pass: Compute gradients\n        loss.backward()\n\n        # Gradient clipping: Prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad_norm)\n\n        # Optimizer step: Update model weights\n        optimizer.step()\n        # --- End Training Step ---\n\n        # --- Logging ---\n        current_loss = loss.item() # Get scalar loss value\n        total_loss += current_loss\n        # Update progress bar postfix with current and average loss\n        progress_bar.set_postfix(loss=f\"{current_loss:.4f}\", avg_loss=f\"{total_loss / (batch_idx + 1):.4f}\")\n\n    # Return average loss for the epoch\n    return total_loss / num_batches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T10:55:00.441142Z","iopub.execute_input":"2025-03-31T10:55:00.441422Z","iopub.status.idle":"2025-03-31T10:55:00.447450Z","shell.execute_reply.started":"2025-03-31T10:55:00.441401Z","shell.execute_reply":"2025-03-31T10:55:00.446569Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### training","metadata":{}},{"cell_type":"code","source":"\n# Check if dataset is populated before starting training\nif 'dataset' not in locals() or len(dataset) == 0:\n    print(\"ERROR: Dataset not created or is empty. Cannot start training.\")\n    print(\"Please ensure the data loading and tokenization steps ran successfully.\")\nelif 'model' not in locals() or model is None:\n    print(\"ERROR: Model not initialized. Cannot start training.\")\nelse:\n    print(\"Starting training...\")\n    print(f\"  Epochs: {config.epochs}\")\n    print(f\"  Batch size: {config.batch_size}\")\n    print(f\"  Max sequence length (tokens): {config.max_seq_len}\")\n    print(f\"  Number of batches per epoch: {len(dataloader)}\")\n    print(f\"  Device: {config.device}\")\n    train_start_time = time.time()\n\n    training_losses = [] # Store average loss per epoch\n\n    # --- Main Training Loop ---\n    for epoch in range(config.epochs):\n        epoch_start_time = time.time()\n\n        # Run one epoch of training\n        avg_train_loss = train_epoch(model, dataloader, optimizer, criterion, config, epoch)\n        training_losses.append(avg_train_loss)\n\n        epoch_duration = time.time() - epoch_start_time\n        print(f\"Epoch {epoch+1}/{config.epochs} | Train Loss: {avg_train_loss:.4f} | Time: {epoch_duration:.2f}s\")\n\n        # Save model periodically to avoid losing progress on long runs\n        if (epoch + 1) % 5 == 0 or epoch == config.epochs - 1: # Save every 5 epochs and at the end\n            chkpt_path = f\"t5_{config.sp_model_prefix}_epoch_{epoch+1}.pt\"\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_train_loss,\n                'config': vars(config), # Save config for reproducibility\n                'sp_model_prefix': config.sp_model_prefix # Need this to load tokenizer later\n            }, chkpt_path)\n            print(f\"Checkpoint saved to {chkpt_path}\")\n\n    # --- Training Finished ---\n    train_duration = time.time() - train_start_time\n    print(f\"\\nTraining finished.\")\n    print(f\"Total Training Time: {train_duration // 60:.0f}m {train_duration % 60:.0f}s\")\n    if training_losses:\n        print(f\"Final Average Training Loss: {training_losses[-1]:.4f}\")\n    final_model_path = f\"t5_{config.sp_model_prefix}_final.pt\"\n    try:\n        torch.save({\n            'model_state_dict': model.state_dict(),\n            'config': vars(config), \n            'sp_model_prefix': config.sp_model_prefix \n        }, final_model_path)\n        print(f\"\\nFinal model state dictionary saved to '{final_model_path}'\")\n        print(f\"-> IMPORTANT: Keep SentencePiece files '{config.sp_model_prefix}.model' and '{config.sp_model_prefix}.vocab' alongside this checkpoint file to be able to reload the model and tokenizer later.\")\n    except Exception as e:\n        print(f\"\\nError saving final model: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T11:02:24.249493Z","iopub.execute_input":"2025-03-31T11:02:24.249915Z","iopub.status.idle":"2025-03-31T13:53:43.047678Z","shell.execute_reply.started":"2025-03-31T11:02:24.249877Z","shell.execute_reply":"2025-03-31T13:53:43.046305Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n  Epochs: 30\n  Batch size: 32\n  Max sequence length (tokens): 128\n  Number of batches per epoch: 1357\n  Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1/30 | Train Loss: 0.6501 | Time: 342.31s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/30 | Train Loss: 0.1604 | Time: 342.24s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/30 | Train Loss: 0.0788 | Time: 342.30s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/30 | Train Loss: 0.0530 | Time: 342.13s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/30 | Train Loss: 0.0401 | Time: 342.54s\nCheckpoint saved to t5_ts_spm_bpe_epoch_5.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/30 | Train Loss: 0.0319 | Time: 341.87s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/30 | Train Loss: 0.0269 | Time: 341.94s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/30 | Train Loss: 0.0231 | Time: 341.90s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/30 | Train Loss: 0.0200 | Time: 341.95s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/30 | Train Loss: 0.0178 | Time: 341.98s\nCheckpoint saved to t5_ts_spm_bpe_epoch_10.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/30 | Train Loss: 0.0162 | Time: 343.16s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/30 | Train Loss: 0.0145 | Time: 343.55s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/30 | Train Loss: 0.0133 | Time: 343.51s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/30 | Train Loss: 0.0121 | Time: 342.71s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/30 | Train Loss: 0.0113 | Time: 342.44s\nCheckpoint saved to t5_ts_spm_bpe_epoch_15.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/30 | Train Loss: 0.0103 | Time: 342.71s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/30 | Train Loss: 0.0096 | Time: 342.71s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/30 | Train Loss: 0.0091 | Time: 342.57s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/30 | Train Loss: 0.0084 | Time: 343.31s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/30 | Train Loss: 0.0079 | Time: 343.29s\nCheckpoint saved to t5_ts_spm_bpe_epoch_20.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21/30 | Train Loss: 0.0071 | Time: 343.66s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22/30 | Train Loss: 0.0067 | Time: 343.30s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23/30 | Train Loss: 0.0062 | Time: 342.26s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 24/30 | Train Loss: 0.0058 | Time: 341.99s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 25/30 | Train Loss: 0.0055 | Time: 341.70s\nCheckpoint saved to t5_ts_spm_bpe_epoch_25.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 26/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7216f7e94cbb4696811dc2ec486f8968"}},"metadata":{}},{"name":"stdout","text":"Epoch 26/30 | Train Loss: 0.0052 | Time: 341.54s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 27/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ebad5f1b514549950eb6e7868ebd65"}},"metadata":{}},{"name":"stdout","text":"Epoch 27/30 | Train Loss: 0.0048 | Time: 341.99s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 28/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77dc0e54d9d542b8b1ca231dca486e96"}},"metadata":{}},{"name":"stdout","text":"Epoch 28/30 | Train Loss: 0.0048 | Time: 341.70s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 29/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 29/30 | Train Loss: 0.0045 | Time: 342.14s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 30/30 Training:   0%|          | 0/1357 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 30/30 | Train Loss: 0.0041 | Time: 342.31s\nCheckpoint saved to t5_ts_spm_bpe_epoch_30.pt\n\nTraining finished.\nTotal Training Time: 171m 18s\nFinal Average Training Loss: 0.0041\n\nFinal model state dictionary saved to 't5_ts_spm_bpe_final.pt'\n-> IMPORTANT: Keep SentencePiece files 'ts_spm_bpe.model' and 'ts_spm_bpe.vocab' alongside this checkpoint file to be able to reload the model and tokenizer later.\n","output_type":"stream"}],"execution_count":99},{"cell_type":"markdown","source":"### generate examples","metadata":{}},{"cell_type":"code","source":"\ndef generate(model, tokenizer: SentencePieceTokenizer, prompt, max_length=50, config=config,\n             temperature=1.0, top_p=0.9): # Added temp and top_p\n    \"\"\"Generates text autoregressively using nucleus sampling.\"\"\"\n    model.eval()\n    device = config.device\n    pad_token_id = tokenizer.pad_token_id\n    bos_token_id = tokenizer.bos_token_id\n    eos_token_id = tokenizer.eos_token_id\n    d_model = config.d_model\n    scale_emb = math.sqrt(d_model)\n\n    print(f\"\\n--- Generating from prompt: '{prompt}' (temp={temperature}, top_p={top_p}) ---\")\n\n    # Prepare prompt\n    prompt_tokens_ids = tokenizer.encode(prompt, add_bos=True, add_eos=False)\n    input_ids = torch.tensor([prompt_tokens_ids], dtype=torch.long).to(device)\n\n    # --- Encoder Pass (Once) ---\n    with torch.no_grad():\n        src_padding_mask = (input_ids == pad_token_id)\n        src_emb = model.shared_embedding(input_ids) * scale_emb\n        memory = model.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n        memory_key_padding_mask = src_padding_mask\n\n    # --- Decoder Autoregressive Loop ---\n    decoder_input_ids = torch.tensor([[bos_token_id]], dtype=torch.long).to(device) # Start with BOS\n    generated_token_ids = []\n\n    print(f\"Generating (max_length={max_length}): \", end='')\n    for i in range(max_length):\n        with torch.no_grad():\n            tgt_seq_len = decoder_input_ids.size(1)\n            _, tgt_padding_mask, _, tgt_mask = model.create_masks(decoder_input_ids, decoder_input_ids)\n\n            # --- Get Logits ---\n            # Use the simple model forward (no attention weights needed)\n            logits = model(src_ids=input_ids, tgt_ids=decoder_input_ids,\n                           src_padding_mask=src_padding_mask,\n                           tgt_padding_mask=tgt_padding_mask,\n                           memory_key_padding_mask=memory_key_padding_mask,\n                           tgt_mask=tgt_mask)\n\n            last_token_logits = logits[:, -1, :] # Get logits for the very last position\n\n            # Apply temperature\n            if temperature != 1.0:\n                last_token_logits = last_token_logits / temperature\n\n            # Calculate probabilities\n            probs = F.softmax(last_token_logits, dim=-1) # Shape: (1, vocab_size)\n\n            # Sort probabilities and indices\n            sorted_probs, sorted_indices = torch.sort(probs, descending=True) # Shape: (1, vocab_size)\n\n            # Calculate cumulative probabilities\n            cumulative_probs = torch.cumsum(sorted_probs, dim=-1) # Shape: (1, vocab_size)\n\n            # Create mask for probabilities NOT in the nucleus\n            sorted_indices_to_remove = cumulative_probs > top_p # Shape: (1, vocab_size)\n            # Shift the mask to the right to keep the first token above the threshold\n            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n            sorted_indices_to_remove[..., 0] = 0 # Always keep the most likely token\n\n            # Create a mask in the original indices space\n            indices_to_remove = torch.zeros_like(probs, dtype=torch.bool).scatter_(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n\n            # Zero out probabilities of tokens to remove (outside the nucleus)\n            probs[indices_to_remove] = 0\n            # torch.multinomial requires non-negative weights summing to non-zero.\n            # If all probabilities become zero (e.g., top_p=0), multinomial will error.\n            # We can handle this edge case, but with reasonable top_p it's unlikely.\n            # Also, multinomial works with unnormalized weights, so explicit re-normalization isn't strictly required.\n            if torch.sum(probs) == 0: # Handle case where all probs got zeroed (e.g. top_p=0)\n                 next_token_id = torch.argmax(last_token_logits, dim=-1).unsqueeze(0) # Fallback to greedy\n                 print(\"[Warning: All probs zeroed, falling back to greedy] \", end='')\n            else:\n                 next_token_id = torch.multinomial(probs, num_samples=1) # Shape: (1, 1)\n\n\n            # --- Stopping Condition ---\n            if next_token_id.item() == eos_token_id:\n                print(\" (EOS)\", end='')\n                break\n\n            # --- Store and Update ---\n            generated_token_ids.append(next_token_id.item())\n            # Detach is not strictly needed here as we are in no_grad context, but good practice\n            decoder_input_ids = torch.cat([decoder_input_ids, next_token_id.detach()], dim=1)\n            try:\n                 piece = tokenizer.processor.id_to_piece(next_token_id.item())\n                 print(piece.replace(' ', ' '), end='', flush=True)\n            except IndexError:\n                 print(f\" [Invalid ID: {next_token_id.item()}] \", end='', flush=True)\n\n    print() \n\n    generated_text = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:57:03.847760Z","iopub.execute_input":"2025-03-31T13:57:03.848100Z","iopub.status.idle":"2025-03-31T13:57:03.859447Z","shell.execute_reply.started":"2025-03-31T13:57:03.848070Z","shell.execute_reply":"2025-03-31T13:57:03.858524Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"\nif 'model' in locals() and model is not None and \\\n   'tokenizer' in locals() and tokenizer is not None:\n    print(\"\\n--- Running Generation Examples (with Nucleus Sampling) ---\")\n\n    prompts = [\n        \"Taylor Swift was born in\",\n        \"The album Fearless featured the song\",\n        \"Folklore and Evermore explored\",\n        \"Her songwriting is known for\",\n        \"The Eras Tour became the\"\n    ]\n    generation_max_len = 75 # Number of tokens to generate\n\n    sampling_temp = 0.7   # Lower -> more deterministic; Higher -> more random\n    sampling_top_p = 0.9  # Probability mass to keep (e.g., 0.9 means keep top 90%)\n\n    for prompt in prompts:\n        start_gen_time = time.time()\n        # Call generate with sampling parameters\n        generated_text = generate(model, tokenizer, prompt,\n                                  max_length=generation_max_len,\n                                  config=config,\n                                  temperature=sampling_temp,\n                                  top_p=sampling_top_p)\n        gen_duration = time.time() - start_gen_time\n        print(f\"-> Generated ({gen_duration:.2f}s): '{generated_text}'\")\n        print(f\"-> Full Text: '{prompt}{generated_text}'\")\n        print(\"-\" * 30)\n\nelse:\n    print(\"\\nModel or tokenizer not found or not initialized.\")\n    print(\"Please ensure the previous cells, including training, have been run successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T13:57:07.786813Z","iopub.execute_input":"2025-03-31T13:57:07.787099Z","iopub.status.idle":"2025-03-31T13:57:15.787367Z","shell.execute_reply.started":"2025-03-31T13:57:07.787076Z","shell.execute_reply":"2025-03-31T13:57:15.786645Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Generation Examples (with Nucleus Sampling) ---\n\n--- Generating from prompt: 'Taylor Swift was born in' (temp=0.7, top_p=0.9) ---\nGenerating (max_length=75): ▁Casticles:▁Taylor▁Swift's▁'Miss▁Americana'▁Is▁What▁Youth▁Annual▁Grammy▁Awards\".▁Billboard.▁Archived▁from▁the▁original▁on▁May▁13,▁2020.▁Retrieved▁May▁13,▁2020.▁\"9▁Things▁You▁Might▁Have▁Missed▁in▁Taylor▁Swift's▁Netflix▁Concert▁Film\".▁E!▁News.▁December▁28,▁2021.▁Retrieved▁May▁13,▁2020.▁Hiatt,▁Brian▁(September▁30,▁2019).▁\"\n-> Generated (1.71s): 'Casticles: Taylor Swift's 'Miss Americana' Is What Youth Annual Grammy Awards\". Billboard. Archived from the original on May 13, 2020. Retrieved May 13, 2020. \"9 Things You Might Have Missed in Taylor Swift's Netflix Concert Film\". E! News. December 28, 2021. Retrieved May 13, 2020. Hiatt, Brian (September 30, 2019). \"'\n-> Full Text: 'Taylor Swift was born inCasticles: Taylor Swift's 'Miss Americana' Is What Youth Annual Grammy Awards\". Billboard. Archived from the original on May 13, 2020. Retrieved May 13, 2020. \"9 Things You Might Have Missed in Taylor Swift's Netflix Concert Film\". E! News. December 28, 2021. Retrieved May 13, 2020. Hiatt, Brian (September 30, 2019). \"'\n------------------------------\n\n--- Generating from prompt: 'The album Fearless featured the song' (temp=0.7, top_p=0.9) ---\nGenerating (max_length=75): ▁Copy▁at▁Indie▁Record▁Streams▁in▁2005,▁Swift▁caught▁the▁attention▁of▁Scott▁and▁supported▁by▁the▁Guinness▁World▁Records,[426]▁but▁first▁efforts\".▁The▁Guardian.▁Archived▁from▁the▁original▁on▁June▁9,▁2019.▁Retrieved▁June▁16,▁2020.▁Lipshutz,▁Jason▁(December▁11,▁2019).▁\"Billboard▁Woman▁of▁the▁Year▁Taylor▁Swift:▁'I▁Do▁Weissongs▁First▁Week\".▁Billboard.\n-> Generated (1.54s): 'Copy at Indie Record Streams in 2005, Swift caught the attention of Scott and supported by the Guinness World Records,[426] but first efforts\". The Guardian. Archived from the original on June 9, 2019. Retrieved June 16, 2020. Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Year Taylor Swift: 'I Do Weissongs First Week\". Billboard.'\n-> Full Text: 'The album Fearless featured the songCopy at Indie Record Streams in 2005, Swift caught the attention of Scott and supported by the Guinness World Records,[426] but first efforts\". The Guardian. Archived from the original on June 9, 2019. Retrieved June 16, 2020. Lipshutz, Jason (December 11, 2019). \"Billboard Woman of the Year Taylor Swift: 'I Do Weissongs First Week\". Billboard.'\n------------------------------\n\n--- Generating from prompt: 'Folklore and Evermore explored' (temp=0.7, top_p=0.9) ---\nGenerating (max_length=75): ▁Cremer▁register▁and▁\"Safe▁&▁Sound\"▁won▁the▁Grammy▁Awards\".▁National▁Music▁Publishers'▁Association.▁May▁24,▁2021.▁Retrieved▁May▁24,▁2021.▁•▁\"Taylor▁Swift▁wins▁IFPI's▁2022▁Global▁Recording▁Artist▁Of▁The▁Year▁three▁times▁at▁the▁529.▁Wars▁like▁Patty,▁ticket▁sales▁or▁just▁cultural▁impact.\"▁—▁and▁So▁Much▁More▁Popular▁Music▁Group;▁McGrath▁2023,\n-> Generated (1.57s): 'Cremer register and \"Safe & Sound\" won the Grammy Awards\". National Music Publishers' Association. May 24, 2021. Retrieved May 24, 2021. • \"Taylor Swift wins IFPI's 2022 Global Recording Artist Of The Year three times at the 529. Wars like Patty, ticket sales or just cultural impact.\" — and So Much More Popular Music Group; McGrath 2023,'\n-> Full Text: 'Folklore and Evermore exploredCremer register and \"Safe & Sound\" won the Grammy Awards\". National Music Publishers' Association. May 24, 2021. Retrieved May 24, 2021. • \"Taylor Swift wins IFPI's 2022 Global Recording Artist Of The Year three times at the 529. Wars like Patty, ticket sales or just cultural impact.\" — and So Much More Popular Music Group; McGrath 2023,'\n------------------------------\n\n--- Generating from prompt: 'Her songwriting is known for' (temp=0.7, top_p=0.9) ---\nGenerating (max_length=75): ▁Cutual▁property▁in▁new▁musicians,[52][603]▁and▁signed▁multi-year▁contract▁with▁AT&T▁and▁Capital▁One.[563][564]▁She▁was▁a▁spokesperson▁for▁the▁National▁Hockey▁League's▁Nashville▁Predators▁and▁Sony▁Cyber-shot▁digital▁cameras,[565][566]▁She▁has▁been▁credited▁with▁making▁a▁sav\n-> Generated (1.61s): 'Cutual property in new musicians,[52][603] and signed multi-year contract with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League's Nashville Predators and Sony Cyber-shot digital cameras,[565][566] She has been credited with making a sav'\n-> Full Text: 'Her songwriting is known forCutual property in new musicians,[52][603] and signed multi-year contract with AT&T and Capital One.[563][564] She was a spokesperson for the National Hockey League's Nashville Predators and Sony Cyber-shot digital cameras,[565][566] She has been credited with making a sav'\n------------------------------\n\n--- Generating from prompt: 'The Eras Tour became the' (temp=0.7, top_p=0.9) ---\nGenerating (max_length=75): ▁Cinematicles:▁These▁albums▁speaker▁to▁receive▁birth\".▁The▁Guardian.▁Retrieved▁August▁2,▁2023.▁Lynch,▁Joe▁(March▁5,▁2021).▁\"Taylor▁Swift's▁'Bad▁Blood'▁Blasts▁to▁No.▁1▁on▁Hot▁100\".▁Billboard.▁Retrieved▁May▁28,▁2021.▁\"Taylor▁Swift▁–▁Fearless▁Fearless▁Fearless▁Tour\".▁The▁New▁York▁Times.▁Archived▁from▁the▁original▁on▁September▁11,\n-> Generated (1.56s): 'Cinematicles: These albums speaker to receive birth\". The Guardian. Retrieved August 2, 2023. Lynch, Joe (March 5, 2021). \"Taylor Swift's 'Bad Blood' Blasts to No. 1 on Hot 100\". Billboard. Retrieved May 28, 2021. \"Taylor Swift – Fearless Fearless Fearless Tour\". The New York Times. Archived from the original on September 11,'\n-> Full Text: 'The Eras Tour became theCinematicles: These albums speaker to receive birth\". The Guardian. Retrieved August 2, 2023. Lynch, Joe (March 5, 2021). \"Taylor Swift's 'Bad Blood' Blasts to No. 1 on Hot 100\". Billboard. Retrieved May 28, 2021. \"Taylor Swift – Fearless Fearless Fearless Tour\". The New York Times. Archived from the original on September 11,'\n------------------------------\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}